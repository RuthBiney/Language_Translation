{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RuthBiney/Language_Translation/blob/main/Language_Translation_with_RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lViwpIpWYTOQ"
      },
      "source": [
        "#1. Tokenization and Alignment\n",
        "##Steps:\n",
        "1. Read the data files: Load both the English and Twi files.\n",
        "\n",
        "2. Align the sentences: Ensure that each English sentence has a corresponding Twi sentence.\n",
        "3. Tokenize the sentences: Split each sentence into individual words."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n"
      ],
      "metadata": {
        "id": "DUDvw0oU0slk"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "source": [
        "# Step 1: Load the dataset with logging of sentence count\n",
        "def load_data(english_file_path, twi_file_path):\n",
        "    with open(english_file_path, 'r', encoding='utf-8', errors='replace') as english_file:\n",
        "        english_sentences = english_file.readlines()\n",
        "\n",
        "    with open(twi_file_path, 'r', encoding='utf-8', errors='replace') as twi_file:\n",
        "        twi_sentences = twi_file.readlines()\n",
        "\n",
        "    english_count = len(english_sentences)\n",
        "    twi_count = len(twi_sentences)\n",
        "\n",
        "    # Log the sentence counts\n",
        "    print(f\"Number of English sentences: {english_count}\")\n",
        "    print(f\"Number of Twi sentences: {twi_count}\")\n",
        "\n",
        "    # Raise an error if the counts do not match\n",
        "    if english_count != twi_count:\n",
        "        print(f\"Mismatch! English sentences: {english_count}, Twi sentences: {twi_count}\")\n",
        "        # Optionally handle the mismatch, e.g., by padding/trimming\n",
        "        # Here we'll pad the shorter list with empty strings\n",
        "        if english_count > twi_count:\n",
        "            twi_sentences += [''] * (english_count - twi_count)\n",
        "        else:\n",
        "            english_sentences += [''] * (twi_count - english_count)\n",
        "\n",
        "    return english_sentences, twi_sentences\n",
        "\n",
        "# File paths to your dataset\n",
        "english_file_path = '/content/english'\n",
        "twi_file_path = '/content/twi'\n",
        "\n",
        "# Load and preprocess the data\n",
        "english_sentences, twi_sentences = load_data(english_file_path, twi_file_path)\n",
        "\n",
        "# Check first few sentence pairs\n",
        "for i in range(3):\n",
        "    print(f\"English: {english_sentences[i]}\")\n",
        "    print(f\"Twi: {twi_sentences[i]}\\n\")\n"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CiK1icNLxJjW",
        "outputId": "31b274c0-2f7d-402a-a3d6-b4250f053c4d"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of English sentences: 976541\n",
            "Number of Twi sentences: 606197\n",
            "Mismatch! English sentences: 976541, Twi sentences: 606197\n",
            "English: “ Oh , Jehovah , Keep My Young Girl Faithful ! ”\n",
            "\n",
            "Twi: “ Oo , Yehowa , Boa Me Babea Kumaa Yi Ma Onni Nokware ! ”\n",
            "\n",
            "\n",
            "English: I WAS born in 1930 in Alsace , France , into an artistic family .\n",
            "\n",
            "Twi: WƆWOO me too abusua a wonim adwinne di mu wɔ Alsace , France , wɔ 1930 mu .\n",
            "\n",
            "\n",
            "English: During the evenings , Father , sitting in his lounge chair , would be reading some books about geography or astronomy .\n",
            "\n",
            "Twi: Ná Papa taa pa twere n’agua mu kenkan asase ho nsɛm anaa ewim nneɛma ho nhoma bi anwummere anwummere .\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-mjG5RgaZ7B"
      },
      "source": [
        "#2. Model Building\n",
        "We will use Recurrent Neural Networks (RNN) to build the translation model. Here's a general outline of the model-building process using TensorFlow and Keras:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "id": "P6Hiu6jua6K4",
        "outputId": "ec6913d9-2a4e-4c90-a103-ce4188aea1bb"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding_2 (\u001b[38;5;33mEmbedding\u001b[0m)              │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_4 (\u001b[38;5;33mLSTM\u001b[0m)                        │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_5 (\u001b[38;5;33mLSTM\u001b[0m)                        │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                      │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)              │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                        │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                        │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "from tensorflow.keras.models import Sequential\n",
        "\n",
        "# Step 1: Define the model\n",
        "def build_translation_model(input_dim, output_dim, input_length):\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(input_dim=input_dim, output_dim=256, input_length=input_length))\n",
        "    model.add(LSTM(512, return_sequences=True))\n",
        "    model.add(LSTM(512))\n",
        "    model.add(Dense(output_dim, activation='softmax'))\n",
        "\n",
        "    return model\n",
        "\n",
        "# Step 2: Compile the model\n",
        "model = build_translation_model(input_dim=10000, output_dim=10000, input_length=100)  # Adjust dimensions based on your data\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Print model summary\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sbbIah1mbJ78"
      },
      "source": [
        "#3. Training and Evaluation\n",
        "For training and evaluating the model:"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1Implement Evaluation with BLEU Score:\n",
        "Add the BLEU score as part of the evaluation. You can use a library like nltk to calculate BLEU:"
      ],
      "metadata": {
        "id": "7xQcyI10HrTx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "from tensorflow.keras.models import Sequential\n",
        "import numpy as np\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "\n",
        "# Function to calculate BLEU score with smoothing\n",
        "def calculate_bleu(reference, candidate):\n",
        "    candidate = [int(np.argmax(c)) for c in candidate[0]]  # Convert NumPy array to list of indices\n",
        "    smoothing = SmoothingFunction().method1  # Use smoothing function to avoid 0 scores for higher n-grams\n",
        "    return sentence_bleu([reference], candidate, smoothing_function=smoothing)\n",
        "\n",
        "# Example: Evaluate your model's output against the Twi references\n",
        "num_samples = min(5, len(tokenized_english))  # Ensure we don't go out of bounds\n",
        "\n",
        "for i in range(num_samples):  # Dynamically set the range based on data size\n",
        "    english_input = tokenized_english[i]\n",
        "    reference_translation = tokenized_twi[i]  # Ground truth in Twi\n",
        "\n",
        "    # Use your trained model to generate a translation\n",
        "    model_output = model.predict(np.array([english_input]))  # Convert list to NumPy array\n",
        "\n",
        "    # Calculate BLEU score\n",
        "    bleu_score = calculate_bleu(reference_translation, model_output)\n",
        "    print(f\"BLEU Score for sentence {i+1}: {bleu_score}\")\n"
      ],
      "metadata": {
        "id": "9dhABSOXHz6E",
        "outputId": "43af4e6e-a963-42e4-d2d5-73fbcb88f645",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "BLEU Score for sentence 1: 0.05372849659117709\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "BLEU Score for sentence 2: 0.069372929071742\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###3.2. Enhancing Data Preprocessing\n",
        "To ensure the dataset is clean and aligned properly, let's add a preprocessing step to handle special characters, punctuation, and consistency between sentence pairs."
      ],
      "metadata": {
        "id": "WYxeqHdgJEg8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# Function to clean and preprocess the sentences\n",
        "def clean_sentence(sentence):\n",
        "    # Convert to lowercase\n",
        "    sentence = sentence.lower()\n",
        "\n",
        "    # Remove special characters and punctuation (keeping standard alphabets and spaces)\n",
        "    sentence = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", sentence)\n",
        "\n",
        "    # Tokenize by splitting on spaces\n",
        "    tokens = sentence.split()\n",
        "\n",
        "    return tokens\n",
        "\n",
        "# Apply cleaning to both English and Twi sentences\n",
        "tokenized_english = [clean_sentence(sentence) for sentence in english_sentences]\n",
        "tokenized_twi = [clean_sentence(sentence) for sentence in twi_sentences]\n"
      ],
      "metadata": {
        "id": "bflYNjLlJIu8"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###3.3. Attention Mechanism (Optional but Useful Enhancement)\n",
        "Adding an attention mechanism can improve the model’s translation quality, especially for longer sentences. Here’s a simplified way to integrate attention into an RNN-based model:"
      ],
      "metadata": {
        "id": "FIKCKAOVJWiu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Input, Attention, Concatenate\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "# Function to build translation model with attention\n",
        "def build_translation_model_with_attention(input_dim, output_dim, input_length):\n",
        "    # Encoder input\n",
        "    encoder_input = Input(shape=(input_length,))\n",
        "    encoder_embedding = Embedding(input_dim=input_dim, output_dim=256)(encoder_input)\n",
        "\n",
        "    # Encoder LSTM\n",
        "    encoder_lstm = LSTM(512, return_sequences=True, return_state=True)\n",
        "    encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)\n",
        "\n",
        "    # Decoder input\n",
        "    decoder_input = Input(shape=(input_length,))\n",
        "    decoder_embedding = Embedding(input_dim=input_dim, output_dim=256)(decoder_input)\n",
        "\n",
        "    # Decoder LSTM\n",
        "    decoder_lstm = LSTM(512, return_sequences=True, return_state=True)\n",
        "    decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=[state_h, state_c])\n",
        "\n",
        "    # Attention layer\n",
        "    attention = Attention()([decoder_outputs, encoder_outputs])\n",
        "\n",
        "    # Combine decoder outputs with attention\n",
        "    decoder_combined_context = Concatenate(axis=-1)([decoder_outputs, attention])\n",
        "\n",
        "    # Final Dense layer\n",
        "    output = Dense(output_dim, activation='softmax')(decoder_combined_context)\n",
        "\n",
        "    # Define the model\n",
        "    model = Model([encoder_input, decoder_input], output)\n",
        "\n",
        "    return model\n",
        "\n",
        "# Build the model\n",
        "model_with_attention = build_translation_model_with_attention(input_dim=10000, output_dim=10000, input_length=100)\n",
        "\n",
        "# Compile the model\n",
        "model_with_attention.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Print the model summary\n",
        "model_with_attention.summary()\n"
      ],
      "metadata": {
        "id": "4wC42m9vJaS3",
        "outputId": "21b9ff1e-dddd-4037-fea3-4ef4d9d03dd2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 578
        }
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_9\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_9\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m       Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to          \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_9             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)            │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ input_layer_10            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)            │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ embedding_11 (\u001b[38;5;33mEmbedding\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │      \u001b[38;5;34m2,560,000\u001b[0m │ input_layer_9[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ embedding_12 (\u001b[38;5;33mEmbedding\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │      \u001b[38;5;34m2,560,000\u001b[0m │ input_layer_10[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ lstm_11 (\u001b[38;5;33mLSTM\u001b[0m)            │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m512\u001b[0m),     │      \u001b[38;5;34m1,574,912\u001b[0m │ embedding_11[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "│                           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,    │                │                        │\n",
              "│                           │ \u001b[38;5;34m512\u001b[0m)]                  │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ lstm_12 (\u001b[38;5;33mLSTM\u001b[0m)            │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m512\u001b[0m),     │      \u001b[38;5;34m1,574,912\u001b[0m │ embedding_12[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],    │\n",
              "│                           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,    │                │ lstm_11[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m1\u001b[0m],         │\n",
              "│                           │ \u001b[38;5;34m512\u001b[0m)]                  │                │ lstm_11[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m2\u001b[0m]          │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ attention (\u001b[38;5;33mAttention\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ lstm_12[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],         │\n",
              "│                           │                        │                │ lstm_11[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ concatenate (\u001b[38;5;33mConcatenate\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m1024\u001b[0m)      │              \u001b[38;5;34m0\u001b[0m │ lstm_12[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],         │\n",
              "│                           │                        │                │ attention[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_10 (\u001b[38;5;33mDense\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m10000\u001b[0m)     │     \u001b[38;5;34m10,250,000\u001b[0m │ concatenate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)              </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">        Param # </span>┃<span style=\"font-weight: bold\"> Connected to           </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_9             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ input_layer_10            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ embedding_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,560,000</span> │ input_layer_9[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ embedding_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,560,000</span> │ input_layer_10[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ lstm_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)            │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>),     │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,574,912</span> │ embedding_11[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "│                           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,    │                │                        │\n",
              "│                           │ <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)]                  │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ lstm_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)            │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>),     │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,574,912</span> │ embedding_12[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],    │\n",
              "│                           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,    │                │ lstm_11[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>],         │\n",
              "│                           │ <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)]                  │                │ lstm_11[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>]          │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ attention (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Attention</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ lstm_12[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],         │\n",
              "│                           │                        │                │ lstm_11[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ concatenate (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)      │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ lstm_12[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],         │\n",
              "│                           │                        │                │ attention[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10000</span>)     │     <span style=\"color: #00af00; text-decoration-color: #00af00\">10,250,000</span> │ concatenate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m18,519,824\u001b[0m (70.65 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">18,519,824</span> (70.65 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m18,519,824\u001b[0m (70.65 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">18,519,824</span> (70.65 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###3.4 Final Steps for Training\n",
        "Ensure that the training code properly tokenizes the sentences and converts them into sequences of integers for the model to understand."
      ],
      "metadata": {
        "id": "cOww1ki_KDIA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Example tokenized English and Twi sentences\n",
        "tokenized_english = [\"Hello, how are you?\", \"What is your name?\", \"I am a data scientist.\"]\n",
        "tokenized_twi = [\"Wo ho te sen?\", \"Wo din de sen?\", \"Meyɛ data scientist.\"]\n",
        "\n",
        "# Prepare tokenizers for both English and Twi\n",
        "english_tokenizer = Tokenizer()\n",
        "twi_tokenizer = Tokenizer()\n",
        "\n",
        "# Fit tokenizers on the sentences\n",
        "english_tokenizer.fit_on_texts(tokenized_english)\n",
        "twi_tokenizer.fit_on_texts(tokenized_twi)\n",
        "\n",
        "# Convert text to sequences of integers\n",
        "english_sequences = english_tokenizer.texts_to_sequences(tokenized_english)\n",
        "twi_sequences = twi_tokenizer.texts_to_sequences(tokenized_twi)\n",
        "\n",
        "# Padding the sequences to ensure uniform length\n",
        "english_sequences = pad_sequences(english_sequences, padding='post')\n",
        "twi_sequences = pad_sequences(twi_sequences, padding='post')\n",
        "\n",
        "# Now you can train the model with the prepared sequences\n",
        "print(\"English sequences:\", english_sequences)\n",
        "print(\"Twi sequences:\", twi_sequences)\n"
      ],
      "metadata": {
        "id": "gkwPAMHlKMiF",
        "outputId": "2ed2c0b7-bcb3-4a5d-b282-57d3a47a4edc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English sequences: [[ 1  2  3  4  0]\n",
            " [ 5  6  7  8  0]\n",
            " [ 9 10 11 12 13]]\n",
            "Twi sequences: [[1 3 4 2]\n",
            " [1 5 6 2]\n",
            " [7 8 9 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras\n"
      ],
      "metadata": {
        "id": "pIC2cyIX-MR_",
        "outputId": "04580155-674d-44fe-b4b9-058f3e4c70d1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (3.4.1)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from keras) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from keras) (1.26.4)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras) (13.8.1)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras) (0.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras) (3.11.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras) (0.12.1)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.10/dist-packages (from keras) (0.4.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from keras) (24.1)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from optree->keras) (4.12.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n"
      ],
      "metadata": {
        "id": "1syjy3Gz-oHi"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "source": [
        "# Step 3: Pad sequences to ensure uniform length (use same max_length for both languages)\n",
        "max_length = max(max(len(seq) for seq in x_train), max(len(seq) for seq in y_train))  # Take the maximum of both\n",
        "\n",
        "# Pad both input (x_train, x_val) and target sequences (y_train, y_val) using the same max_length\n",
        "x_train = pad_sequences(x_train, maxlen=max_length)\n",
        "x_val = pad_sequences(x_val, maxlen=max_length)\n",
        "y_train = pad_sequences(y_train, maxlen=max_length)  # Use the same max_length\n",
        "y_val = pad_sequences(y_val, maxlen=max_length)\n",
        "\n",
        "# Convert lists to NumPy arrays\n",
        "x_train = np.array(x_train)\n",
        "y_train = np.array(y_train)\n",
        "x_val = np.array(x_val)\n",
        "y_val = np.array(y_val)\n",
        "\n",
        "# Check the shapes of your validation data\n",
        "print(\"Shape of x_val:\", x_val.shape)\n",
        "print(\"Shape of y_val:\", y_val.shape)\n",
        "\n",
        "# Step 5: Define the model\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=10000, output_dim=64, input_length=max_length))  # Set input_length for the Embedding layer\n",
        "model.add(LSTM(64, return_sequences=True))  # LSTM layer with return_sequences=True\n",
        "model.add(TimeDistributed(Dense(10000, activation='softmax')))  # TimeDistributed output layer for each time step\n",
        "\n",
        "# Step 6: Compile the model\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Step 7: Train with full data (since the dataset is small)\n",
        "# Remove steps_per_epoch and validation_steps - these are not needed for small datasets and may cause issues\n",
        "history = model.fit(\n",
        "    x_train, y_train,  # Train on full data\n",
        "    epochs=2,\n",
        "    validation_data=(x_val, y_val)\n",
        ")\n",
        "\n",
        "# Step 8: Evaluate the model\n",
        "# Ensure you are evaluating on a reasonable portion of the validation data\n",
        "model.evaluate(x_val, y_val)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "4RJWYz1oESaM",
        "outputId": "91de3d6c-4874-4ac7-ce21-077ba3c8b0d6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of x_val: (1, 5)\n",
            "Shape of y_val: (1, 5)\n",
            "Epoch 1/2\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step - accuracy: 0.0000e+00 - loss: 9.2105 - val_accuracy: 0.0000e+00 - val_loss: 9.2089\n",
            "Epoch 2/2\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 171ms/step - accuracy: 0.3000 - loss: 9.2078 - val_accuracy: 0.0000e+00 - val_loss: 9.2063\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.0000e+00 - loss: 9.2063\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[9.206286430358887, 0.0]"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into training and validation sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "english_train, english_val, twi_train, twi_val = train_test_split(\n",
        "    tokenized_english, tokenized_twi, test_size=0.2\n",
        ")\n",
        "\n",
        "# Convert your data into a format suitable for training, such as sequences of integers\n",
        "# You can use Tokenizer from keras.preprocessing.text to tokenize and convert words into sequences of numbers\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    x_train, y_train,  # Preprocessed input and output sequences\n",
        "    epochs=10,\n",
        "    validation_data=(x_val, y_val)\n",
        ")\n",
        "\n",
        "# Evaluate the model\n",
        "model.evaluate(x_val, y_val)\n"
      ],
      "metadata": {
        "id": "SiU3t_qXHkAk",
        "outputId": "1ff66122-38be-4baf-b0f5-752b98c59681",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - accuracy: 0.4000 - loss: 9.2050 - val_accuracy: 0.0000e+00 - val_loss: 9.2036\n",
            "Epoch 2/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 0.4000 - loss: 9.2022 - val_accuracy: 0.0000e+00 - val_loss: 9.2009\n",
            "Epoch 3/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.5000 - loss: 9.1993 - val_accuracy: 0.0000e+00 - val_loss: 9.1981\n",
            "Epoch 4/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - accuracy: 0.5000 - loss: 9.1963 - val_accuracy: 0.0000e+00 - val_loss: 9.1952\n",
            "Epoch 5/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.5000 - loss: 9.1932 - val_accuracy: 0.0000e+00 - val_loss: 9.1920\n",
            "Epoch 6/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - accuracy: 0.5000 - loss: 9.1899 - val_accuracy: 0.0000e+00 - val_loss: 9.1887\n",
            "Epoch 7/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - accuracy: 0.5000 - loss: 9.1864 - val_accuracy: 0.0000e+00 - val_loss: 9.1851\n",
            "Epoch 8/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - accuracy: 0.5000 - loss: 9.1827 - val_accuracy: 0.0000e+00 - val_loss: 9.1812\n",
            "Epoch 9/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - accuracy: 0.5000 - loss: 9.1787 - val_accuracy: 0.0000e+00 - val_loss: 9.1770\n",
            "Epoch 10/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step - accuracy: 0.5000 - loss: 9.1744 - val_accuracy: 0.0000e+00 - val_loss: 9.1724\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.0000e+00 - loss: 9.1724\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[9.172381401062012, 0.0]"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4.Save and Document\n",
        "Once the model is trained, save it and document the process:"
      ],
      "metadata": {
        "id": "eHLp5G3E9Hhz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model in HDF5 format\n",
        "model.save('twi_translation_model.h5')\n"
      ],
      "metadata": {
        "id": "pmLbIrXC9LeN",
        "outputId": "006a0176-c5d7-4618-8c7c-592338293ea9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP7hrlAVpr0ivHbP4bB1UFX",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}